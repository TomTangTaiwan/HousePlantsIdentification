{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMxma/aavcd8x9iehaZv8qz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Import packages and libraries"],"metadata":{"id":"1REWMa249TeN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3K1cnJnw87fX"},"outputs":[],"source":["import os\n","import glob\n","import torch\n","import urllib.request\n","import cv2\n","\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from torch import nn\n","from torch import optim\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.io import read_image\n","from sklearn.metrics import confusion_matrix, f1_score, classification_report\n","\n","from google.colab import output\n","output.enable_custom_widget_manager()\n","from tqdm import tqdm, trange\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n"]},{"cell_type":"markdown","source":["## Reload the best model"],"metadata":{"id":"0L-HxJiL9Xzd"}},{"cell_type":"code","source":["# Specify which file would like to retrieve\n","log_file_path = '/content/gdrive/MyDrive/Colab Notebooks/Training_Log/' + 'CY' + '/epoch_10_2022-08-30 10:38:20.984264.pt'"],"metadata":{"id":"6Zadkku99Zcj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Datasets and parameters\n","img_size = 256\n","\n","path_test = '/content/gdrive/MyDrive/Colab Notebooks/Final_data/test'\n","\n","folders = os.listdir(path_test)\n","folders.sort()\n","label_dict = dict()\n","for id, folder in enumerate(folders):\n","  label_dict[folder] = id\n","\n","n_class = len(label_dict)"],"metadata":{"id":"OztImbW_9ayl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model setting\n","model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n","model.fc = nn.Linear(model.fc.in_features, n_class)\n","\n","# Move the model to GPU\n","device = torch.device(\"cuda:0\")\n","model = model.to(device)\n","\n","# Load model\n","checkpoint = torch.load(log_file_path)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']\n"],"metadata":{"id":"01jaczzI9cH5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load validation/testing data"],"metadata":{"id":"Ay0Ti6f-9eC1"}},{"cell_type":"code","source":["class GreenFingerDataset(Dataset):\n","    def __init__(self, data_path, label_dict, oversample=False, transforms=None):\n","        # retrieve jpg file path from directory: training/validation/testing\n","        self.files = sorted(glob.glob(os.path.join(data_path, \"*/*.jpg\")))\n","        # convert jpg file path to dataframe and attach true label\n","        self.df = pd.DataFrame(\n","            dict(\n","                cat=[f.split('/')[-2] for f in self.files],\n","                image_path=self.files\n","            )\n","        )\n","        # copy dataframe\n","        self.df_oversampled = self.df.copy()\n","        # process oversample function\n","        if oversample:\n","            self.oversample()\n","        # initialize transform procedure\n","        self.transforms = transforms\n","        # initialize true label numbering\n","        self.label_dict = label_dict\n","\n","    def oversample(self):\n","        \"\"\"\n","        This functions can be called or \n","        initialized automatically when the oversample=True\n","        \"\"\"\n","        # Random sampling til the # of sample matches to the # of largest category\n","        cats = self.df.cat.drop_duplicates().to_numpy()\n","        cat_sizes = []\n","\n","        for c in cats:\n","            n = len(self.df.query(f\"cat == '{c}'\"))\n","            cat_sizes.append(n)\n","\n","        cat_sizes = np.array(cat_sizes)\n","        dfs = []\n","        n_majority = np.max(cat_sizes)\n","\n","        for i, c in enumerate(cats):\n","            df_cat = self.df.query(f\"cat == '{c}'\").sample(frac=n_majority/cat_sizes[i], replace=True)\n","            dfs.append(df_cat)\n","\n","        self.df_oversampled = pd.concat(dfs, axis=0).reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df_oversampled)\n","\n","    def __getitem__(self, idx):\n","        img_src, cat = self.df_oversampled.iloc[idx][[\"image_path\", \"cat\"]]\n","        # What's the difference between torchvision's readimage and PIL's Image.open?\n","        x = Image.open(img_src)\n","        # x = read_image(img_src)\n","        if self.transforms:\n","            x = self.transforms(x)\n","        x = x.float()\n","        if len(x.size()) == 2:\n","            # when it's multi channels, x.size() shows (Channel, Heigh, Weight)\n","            # when channel is 1, x.size() shows (Heigh, Weight) w/o channel\n","            x = x.unsqueeze(0) \n","            x = torch.cat((x, x, x), axis=0)\n","        y = self.label_dict[cat]\n","        y = torch.Tensor([y]).long()\n","        \n","        return x, y"],"metadata":{"id":"fNwnb25n9g_J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform for Testing Dataset\n","transforms_test = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Resize(img_size),\n","     transforms.CenterCrop(img_size),\n","     transforms.Normalize(\n","         mean=[0.485, 0.456, 0.406],\n","         std=[0.229, 0.224, 0.225])])\n","\n","# Testing Dataset\n","ds_test = GreenFingerDataset(\n","    path_test,\n","    label_dict,\n","    transforms=transforms_test\n",")\n","print(\"ds_test\", len(ds_test))"],"metadata":{"id":"xG3iXXhh9iqJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Validation accuracy"],"metadata":{"id":"vLTuYQ0S9j7A"}},{"cell_type":"code","source":["model.eval()\n","\n","batch_size = 128\n","\n","dl_test = DataLoader(\n","    ds_test,\n","    shuffle=False,\n","    batch_size=batch_size,\n","    drop_last=False,\n",")\n","\n","predictions = []\n","\n","with torch.no_grad():\n","    for (x, y) in tqdm(dl_test):\n","        x = x.to(device)\n","        y = y.to(device)\n","        output = model(x)\n","        predictions.append(output.cpu().numpy())\n","    # Compute Validation Accuracy\n","    preds_test = np.argmax(np.concatenate(predictions, axis=0), axis=1)\n","    y_test = np.concatenate([item[1].numpy() for item in ds_test])\n","    acc_test = np.sum(preds_test == y_test) / len(y_test)\n","    print(f'Test Acc: {acc_test}')"],"metadata":{"id":"1opAT3y99mtA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Confusion matrix"],"metadata":{"id":"pcKhEfh69rVe"}},{"cell_type":"code","source":["inv_label_dict = {v: k for k, v in label_dict.items()}\n","\n","confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, preds_test)).rename(columns=inv_label_dict, index=inv_label_dict)\n","\n","confusion_matrix_df = confusion_matrix_df.round(2)\n","\n","# Plotting\n","fig, ax = plt.subplots(figsize=(14,13))         \n","sns.heatmap(confusion_matrix_df, annot=True, ax=ax, linewidths=0, cbar=False, cmap='mako')\n","\n","plt.setp(ax.get_xticklabels(), rotation=45, ha='right', color='white', fontsize=15)\n","plt.setp(ax.get_yticklabels(), color='white', fontsize=15)\n","\n","plt.xlabel(\"prediction\", fontsize=30, color='white')\n","plt.ylabel(\"true label\", fontsize=30, color='white')"],"metadata":{"id":"MXYlb2fj9qtD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Confusion matrix - Recall"],"metadata":{"id":"8QO3l3J79v92"}},{"cell_type":"code","source":["inv_label_dict = {v: k for k, v in label_dict.items()}\n","\n","confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, preds_test, normalize=\"true\")).rename(columns=inv_label_dict, index=inv_label_dict)\n","# confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, preds_test, normalize=\"pred\")).rename(columns=inv_label_dict, index=inv_label_dict)\n","# confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, preds_test)).rename(columns=inv_label_dict, index=inv_label_dict)\n","\n","confusion_matrix_df = confusion_matrix_df.round(2)\n","\n","# Plotting\n","fig, ax = plt.subplots(figsize=(14,13))         \n","sns.heatmap(confusion_matrix_df, annot=True, ax=ax, linewidths=0, cbar=False, cmap='mako')\n","\n","plt.setp(ax.get_xticklabels(), rotation=45, ha='right', color='white', fontsize=15)\n","plt.setp(ax.get_yticklabels(), color='white', fontsize=15)\n","\n","plt.xlabel(\"prediction\", fontsize=30, color='white')\n","plt.ylabel(\"true label\", fontsize=30, color='white')"],"metadata":{"id":"gE9lX52t9vBg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Confusion matrix - Precision"],"metadata":{"id":"tNAAL3c4914Y"}},{"cell_type":"code","source":["inv_label_dict = {v: k for k, v in label_dict.items()}\n","\n","confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, preds_test, normalize=\"pred\")).rename(columns=inv_label_dict, index=inv_label_dict)\n","\n","confusion_matrix_df = confusion_matrix_df.round(2)\n","\n","# Plotting\n","fig, ax = plt.subplots(figsize=(14,13))         \n","sns.heatmap(confusion_matrix_df, annot=True, ax=ax, linewidths=0, cbar=False, cmap='mako')\n","\n","plt.setp(ax.get_xticklabels(), rotation=45, ha='right', color='white', fontsize=15)\n","plt.setp(ax.get_yticklabels(), color='white', fontsize=15)\n","\n","plt.xlabel(\"prediction\", fontsize=30, color='white')\n","plt.ylabel(\"true label\", fontsize=30, color='white')"],"metadata":{"id":"-iJmbzTX93jf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Confusion matrix: numbers but with normalized color scale"],"metadata":{"id":"QjOMOSkJ95vD"}},{"cell_type":"code","source":["inv_label_dict = {v: k for k, v in label_dict.items()}\n","\n","confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, preds_test, normalize=\"true\")).rename(columns=inv_label_dict, index=inv_label_dict)\n","# confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, preds_test, normalize=\"pred\")).rename(columns=inv_label_dict, index=inv_label_dict)\n","# confusion_matrix_df = pd.DataFrame(confusion_matrix(y_test, preds_test)).rename(columns=inv_label_dict, index=inv_label_dict)\n","\n","confusion_matrix_df_numbers = pd.DataFrame(confusion_matrix(y_test, preds_test)).rename(columns=inv_label_dict, index=inv_label_dict)\n","\n","confusion_matrix_df_numbers = confusion_matrix_df_numbers.round(2)\n","\n","# print(confusion_matrix_df_numbers)\n","\n","# Plotting\n","fig, ax = plt.subplots(figsize=(14,13))         \n","sns.heatmap(confusion_matrix_df, annot=confusion_matrix_df_numbers, ax=ax, linewidths=0, cbar=False, cmap='Blues')\n","\n","plt.setp(ax.get_xticklabels(), rotation=45, ha='right', color='white', fontsize=15)\n","plt.setp(ax.get_yticklabels(), color='white', fontsize=15)\n","\n","plt.xlabel(\"prediction\", fontsize=30, color='white')\n","plt.ylabel(\"true label\", fontsize=30, color='white')"],"metadata":{"id":"HwXvGqbg968Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_f1_dict = sorted(inv_label_dict.items(), reverse=True)\n","plot_f1_dict"],"metadata":{"id":"91DHHbHC98rJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# F1 score\n","f1 = f1_score(y_test, preds_test, average=None)\n","print(f1)\n","\n","w_f1 = f1_score(y_test, preds_test, average='weighted')\n","print(w_f1)\n","\n","\n","fig, ax = plt.subplots(figsize=(8,8))         \n","\n","for i, item in plot_f1_dict:\n","    ax.barh(item, f1[i], color='teal')\n","\n","ax.vlines(np.ones(20), -0.5, 20, color='white', ls='--')\n","ax.vlines(np.ones(20)*0.8, -0.5, 20, color='white', ls='--')\n","\n","plt.setp(ax.get_xticklabels(), color='white', fontsize=15)\n","plt.setp(ax.get_yticklabels(), color='white', fontsize=15)\n","\n","plt.setp(ax.set_xlim(0.65, 1.0))\n","plt.setp(ax.set_ylim(-0.5 , 19.5))\n","\n","\n","ax.patch.set_facecolor('none')\n","\n","for spine in ax.spines.values():\n","        spine.set_edgecolor('white')\n","\n","ax.set_xlabel(\"F1 score\", fontsize=20, color='white')\n","ax.set_ylabel(\"Class\", fontsize=20, color='white')"],"metadata":{"id":"RBNAvAYu9-SY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Show mis-matched targets"],"metadata":{"id":"ZyTf8LFc-ATx"}},{"cell_type":"code","source":["# Initialize transform proceduce for visualizing images\n","\n","transforms_test_for_viz = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Resize(img_size),\n","     transforms.CenterCrop(img_size)  # unify all image as square shape\n","     ])\n","\n","# Initialize dataset for visualization\n","ds_test_for_viz = GreenFingerDataset(\n","    path_test,\n","    label_dict,\n","    transforms=transforms_test_for_viz)\n","\n","print(\"ds_test_for_viz\", len(ds_test_for_viz))\n","\n","# Filter image's indices where preds != y (unmatched items' indices)\n","debug_idxs_test = np.where(preds_test != y_test)\n","\n","# Filter ds_test_for_viz dataframe with unmatched items\n","debug_df_test = ds_test_for_viz.df.filter(items=debug_idxs_test[0], axis=0)\n","\n","# Filter preds array with unmatched items\n","debug_preds_test = preds_test[debug_idxs_test]"],"metadata":{"id":"QjmAsOEt-Bzv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize wrongly predicted images\n","fig = plt.figure()\n","fig.set_size_inches(35, 10)\n","for i in range(len(debug_df_test)):\n","    img_src, cat = debug_df_test.iloc[i][[\"image_path\", \"cat\"]]\n","\n","    wrong_ind = debug_idxs_test[0][i]\n","    img, _ = ds_test_for_viz[wrong_ind]\n","    im = torch.permute(img, [1, 2, 0]).numpy()\n","\n","    pred = debug_preds_test[i]\n","    pred = list(label_dict.keys())[list(label_dict.values()).index(pred)]\n","    plt.subplot(2, 8, i+1)\n","    plt.imshow(im)\n","    plt.title(f'T: {cat} \\n vs\\n P: {pred} ', color='white', fontsize=20)\n","    plt.axis(\"off\")"],"metadata":{"id":"1oLz6zF6-DM6"},"execution_count":null,"outputs":[]}]}