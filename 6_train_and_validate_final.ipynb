{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["QPqZ8Lsj6qSz","8vYkathD6zCs","1cmVunyp7AYb","wTUHFu6b7OYi","slLG8eq07f-J","nLTYqBhz7na2","dGL7XLuG7xW8","ji3Mo2Ds78AR","nvapy0Ww8F9i","wO9mzuQp8fgL"],"authorship_tag":"ABX9TyM836zqUoMMhTAvSIlf2Y2p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Initialize Libraries"],"metadata":{"id":"QPqZ8Lsj6qSz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8yGum-Kq6YWX"},"outputs":[],"source":["import os\n","import glob\n","\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.io import read_image\n","\n","import urllib.request\n","from PIL import Image\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","from google.colab import output\n","output.enable_custom_widget_manager()\n","from tqdm import tqdm, trange\n","\n","from sklearn.metrics import confusion_matrix\n","from datetime import datetime"]},{"cell_type":"markdown","source":["## Mount GDrive"],"metadata":{"id":"8vYkathD6zCs"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"otKGTGhc60-k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize Path for each Dataset"],"metadata":{"id":"1cmVunyp7AYb"}},{"cell_type":"code","source":["path_train = '/content/gdrive/MyDrive/Colab Notebooks/Final_data/train'\n","path_val = '/content/gdrive/MyDrive/Colab Notebooks/Final_data/val'\n","path_test = '/content/gdrive/MyDrive/Colab Notebooks/Final_data/test'"],"metadata":{"id":"M01Mf6wv7LE5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset Pre-Processing"],"metadata":{"id":"wTUHFu6b7OYi"}},{"cell_type":"markdown","source":["Must be built with init, len, and getitem\n","\n","Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n","\n","__len__: https://pytorch.org/docs/master/data.html#torch.utils.data.Dataset"],"metadata":{"id":"9B4YmJ717TBR"}},{"cell_type":"code","source":["class GreenFingerDataset(Dataset):\n","\n","    def __init__(self, data_path, label_dict, oversample=False, transforms=None):\n","        # retrieve jpg file path from directory: training/validation/testing\n","        self.files = sorted(glob.glob(os.path.join(data_path, \"*/*.jpg\")))\n","        # convert jpg file path to dataframe and attach true label\n","        self.df = pd.DataFrame(\n","            dict(\n","                cat=[f.split('/')[-2] for f in self.files],\n","                image_path=self.files\n","            )\n","        )\n","        # copy dataframe\n","        self.df_oversampled = self.df.copy()\n","        # process oversample function\n","        if oversample:\n","            self.oversample()\n","        # initialize transform procedure\n","        self.transforms = transforms\n","        # initialize true label numbering\n","        self.label_dict = label_dict\n","\n","    def oversample(self):\n","        \"\"\"\n","        This functions can be called or \n","        initialized automatically when the oversample=True\n","        \"\"\"\n","        # Random sampling til the # of sample matches to the # of largest category\n","        cats = self.df.cat.drop_duplicates().to_numpy()\n","        cat_sizes = []\n","\n","        for c in cats:\n","            n = len(self.df.query(f\"cat == '{c}'\"))\n","            cat_sizes.append(n)\n","\n","        cat_sizes = np.array(cat_sizes)\n","        dfs = []\n","        n_majority = np.max(cat_sizes)\n","\n","        for i, c in enumerate(cats):\n","            df_cat = self.df.query(f\"cat == '{c}'\").sample(frac=n_majority/cat_sizes[i], replace=True)\n","            dfs.append(df_cat)\n","\n","        self.df_oversampled = pd.concat(dfs, axis=0).reset_index(drop=True)\n","\n","    def __len__(self):\n","        return len(self.df_oversampled)\n","\n","    def __getitem__(self, idx):\n","        img_src, cat = self.df_oversampled.iloc[idx][[\"image_path\", \"cat\"]]\n","        # What's the difference between torchvision's readimage and PIL's Image.open?\n","        x = Image.open(img_src)\n","        # x = read_image(img_src)\n","        if self.transforms:\n","            x = self.transforms(x)\n","        x = x.float()\n","        if len(x.size()) == 2:\n","            # when it's multi channels, x.size() shows (Channel, Heigh, Weight)\n","            # when channel is 1, x.size() shows (Heigh, Weight) w/o channel\n","            x = x.unsqueeze(0) \n","            x = torch.cat((x, x, x), axis=0)\n","        y = self.label_dict[cat]\n","        y = torch.Tensor([y]).long()\n","        \n","        return x, y"],"metadata":{"id":"bo0NaYoJ7PsN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Preview Data Augmentation"],"metadata":{"id":"slLG8eq07f-J"}},{"cell_type":"code","source":["my_transforms = transforms.Compose(\n","    [\n","        transforms.ToTensor(),\n","        transforms.RandomResizedCrop(size=256, scale=(0.2, 1.0), ratio=(1.0, 1.0)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.RandomRotation(degrees=(-10, 10), fill=(0,)),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.2, hue=0.05),\n","    ]\n",")\n","\n","ds_preview = GreenFingerDataset(\n","    path_train,\n","    label_dict,\n","    transforms=my_transforms\n",")\n","print(\"ds\", len(ds_preview))"],"metadata":{"id":"PlEL-f8w7jmw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig = plt.figure(figsize=(20, 15))\n","for i in range(25):\n","    img, _ = ds_preview[1]\n","    plt.subplot(5, 5, i + 1)\n","    # Permute is to re-arrange C x H x W to H x W x C, then we can conver to\n","    # numpy array and visualize via plt\n","    plt.imshow(torch.permute(img, [1, 2, 0]).numpy())\n","    plt.axis(\"off\")"],"metadata":{"id":"OfjJtiiP7lgg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize Neural Network"],"metadata":{"id":"nLTYqBhz7na2"}},{"cell_type":"code","source":["model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n","model.fc = nn.Linear(model.fc.in_features, n_class)\n","model"],"metadata":{"id":"px9O8SC37qf-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define Transform Procedure"],"metadata":{"id":"dGL7XLuG7xW8"}},{"cell_type":"code","source":["# Transform for Training Dataset\n","transforms_train = transforms.Compose(\n","    [\n","        transforms.ToTensor(),\n","        transforms.RandomRotation(degrees=(-10, 10), fill=(0,)),\n","        transforms.RandomResizedCrop(size=256, scale=(0.2, 1.0), ratio=(1.0, 1.0)),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.2, hue=0.05),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        )\n","    ]\n",")"],"metadata":{"id":"mFAcykrC70kh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform for Validation Dataset\n","transforms_val = transforms.Compose(\n","    [\n","        transforms.ToTensor(),\n","        transforms.Resize(256),\n","        transforms.CenterCrop(256), # unify all image as square shape\n","        transforms.Normalize( # tied with pre-trained weights\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        )\n","    ]\n",")"],"metadata":{"id":"CrFHdJRT72K6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Transform for Testing Dataset\n","transforms_test = transforms.Compose(\n","    [\n","        transforms.ToTensor(),\n","        transforms.Resize(256),\n","        transforms.CenterCrop(256),\n","        transforms.Normalize(\n","            mean=[0.485, 0.456, 0.406],\n","            std=[0.229, 0.224, 0.225]\n","        )\n","    ]\n",")"],"metadata":{"id":"YuKaCdBS75du"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Initialize Dataset"],"metadata":{"id":"ji3Mo2Ds78AR"}},{"cell_type":"code","source":["# Training Dataset\n","ds_train = GreenFingerDataset(\n","    path_train,\n","    label_dict,\n","    oversample=True,\n","    transforms=transforms_train\n",")\n","print(\"ds_train\", len(ds_train))"],"metadata":{"id":"S856JhhJ78a3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Validation Dataset\n","ds_val = GreenFingerDataset(\n","    path_val,\n","    label_dict,\n","    transforms=transforms_val\n",")\n","print(\"ds_val\", len(ds_val))"],"metadata":{"id":"7caoCT_K8Bay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Testing Dataset\n","ds_test = GreenFingerDataset(\n","    path_test,\n","    label_dict,\n","    transforms=transforms_test\n",")\n","print(\"ds_test\", len(ds_test))"],"metadata":{"id":"uRWx6qs18DPR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training & Validation"],"metadata":{"id":"nvapy0Ww8F9i"}},{"cell_type":"code","source":["# Initialize parameters\n","n_epoch = 10\n","lr = 2e-4\n","batch_size = 128\n","\n","# seen counter\n","seen = 0\n","\n","# Path for saving the training log\n","user = 'Tom'  # Tom, CY, Zephyr --> Change your name accordingly\n","log_path = os.path.join('/content/gdrive/MyDrive/Colab Notebooks/Training_Log', user)"],"metadata":{"id":"oLyty7Pi8IF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize Data Loader\n","dl_train = DataLoader(\n","    ds_train,\n","    shuffle=True,\n","    batch_size=batch_size,\n","    drop_last=True\n",")\n","dl_val = DataLoader(\n","    ds_val,\n","    shuffle=False,\n","    batch_size=batch_size,\n","    drop_last=False,\n",")"],"metadata":{"id":"KMwOZZwO8Jk-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save & Load Model\n","\n","Reference: https://pytorch.org/tutorials/beginner/saving_loading_models.html"],"metadata":{"id":"7ZHArRAV8L10"}},{"cell_type":"code","source":["# Initialize Loss Function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Initialize Optimizer\n","optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-7)\n","\n","device = torch.device(\"cuda:0\")\n","model = model.to(device)\n","\n","# Initialize Log\n","train_log = []\n","\n","for i in range(n_epoch):\n","    model.train()\n","    # no need to switch the model back to model.train()?\n","    for (x, y) in tqdm(dl_train, desc='Training', position=0, leave=True):\n","        x = x.to(device)\n","        y = y.to(device)\n","        # what's the difference if we put optimizer.zero_grad() to later step?\n","        optimizer.zero_grad()\n","        output = model(x)\n","        loss = loss_fn(output, y[:, 0])\n","        loss.backward()\n","        optimizer.step()\n","        with torch.no_grad():\n","            seen = seen + x.size(0)\n","            acc_train = torch.sum(torch.argmax(output, axis=1) == y[:, 0]) / np.sum(batch_size)\n","            print(f' | Seen {seen} | Loss {loss.item()} | Train Acc: {acc_train}')\n","\n","    # Validate every 1 epochs\n","    if (i+1) % 1 == 0:\n","        model.eval()\n","\n","        predictions = []\n","\n","        with torch.no_grad():\n","            for (x, y) in tqdm(dl_val, desc='Validation', position=0, leave=True):\n","                x = x.to(device)\n","                y = y.to(device)\n","                output = model(x)\n","                # To-Do: Optimize Prediction & Accuracy?\n","                predictions.append(output.cpu().numpy())\n","            # Compute Validation Accuracy\n","            preds = np.argmax(np.concatenate(predictions, axis=0), axis=1)\n","            y = np.concatenate([item[1].numpy() for item in ds_val])\n","            acc_val = np.sum(preds == y) / len(preds)\n","            print(f' | Seen: {seen} | Loss: {loss.item()} | Val Acc: {acc_val}')\n","            # Append Training Log\n","            train_log.append(\n","                      dict(\n","                          ePoch = i+1,\n","                          seen = seen,\n","                          loss_train = loss.item(),\n","                          acc_train = acc_train.item(),\n","                          acc_val = acc_val.item()\n","                      )\n","                  )\n","        torch.save({\n","          'epoch': i+1,\n","          'model_state_dict': model.state_dict(),\n","          'optimizer_state_dict': optimizer.state_dict(),\n","          'loss': loss.item()\n","        },\n","          os.path.join(\n","              # Select your path from above\n","              log_path, f'epoch_{i+1}_{datetime.now()}.pt'\n","          )\n","        )"],"metadata":{"id":"IoD4yt808OpW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.DataFrame(train_log)\n","df_train"],"metadata":{"id":"oia0QqiR8RQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","plt.plot(df_train[\"seen\"], df_train[\"acc_train\"], label=\"training\")\n","plt.plot(df_train[\"seen\"], df_train[\"acc_val\"], label=\"validation\")\n","plt.legend()\n","plt.xlabel(\"iterations\")\n","plt.ylabel(\"accuracy\")\n","plt.show()"],"metadata":{"id":"IHOL_qOt8TAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualize Unmatched Result for Validation Dataset\n","\n","p.s. Use the last validation set"],"metadata":{"id":"wO9mzuQp8fgL"}},{"cell_type":"code","source":["# Initialize transform proceduce for visualizing images\n","transforms_val_for_viz = transforms.Compose(\n","    [\n","        transforms.ToTensor(),\n","        transforms.Resize(128),\n","        transforms.CenterCrop(128),\n","    ]\n",")\n","\n","# Initialize dataset for visualization\n","ds_val_for_viz = GreenFingerDataset(\n","    path_val,\n","    label_dict,\n","    transforms=transforms_val_for_viz\n",")\n","print(\"ds_val_for_viz\", len(ds_val_for_viz))"],"metadata":{"id":"5IfiLRTl8c-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter image's indices where preds != y (unmatched items' indices)\n","debug_idxs = np.where(preds != y)\n","debug_idxs"],"metadata":{"id":"e5VnJ0MQ8hVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter ds_val_for_viz dataframe with unmatched items\n","debug_df = ds_val_for_viz.df.filter(items=debug_idxs[0], axis=0)\n","debug_df"],"metadata":{"id":"6hd1Hr9z8ifi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Filter preds array with unmatched items\n","debug_preds = preds[debug_idxs]\n","debug_preds"],"metadata":{"id":"GGY06cv28kir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize unmatched items\n","\n","fig = plt.figure()\n","fig.set_size_inches(30, 150)\n","for i in range(len(debug_df)):\n","  img_src, cat = debug_df.iloc[i][[\"image_path\", \"cat\"]]\n","  img = Image.open(img_src)\n","  pred = debug_preds[i]\n","  pred = list(label_dict.keys())[list(label_dict.values()).index(pred)]\n","  plt.subplot(16, 5, i+1)\n","  plt.imshow(img)\n","  plt.title(f'P: {pred} \\n vs\\n T: {cat}')\n","  plt.axis(\"off\")"],"metadata":{"id":"Q4chQ1GG8l3Z"},"execution_count":null,"outputs":[]}]}